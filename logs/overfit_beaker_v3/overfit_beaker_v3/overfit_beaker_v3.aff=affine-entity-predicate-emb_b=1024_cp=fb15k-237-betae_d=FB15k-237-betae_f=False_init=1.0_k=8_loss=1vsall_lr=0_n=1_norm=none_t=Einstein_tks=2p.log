2022-07-31 05:04:31,256 INFO     ---------------------------------------------------------------------------------------------
2022-07-31 05:04:31,256 INFO     Geo: cqd
2022-07-31 05:04:31,256 INFO     Data Path: data/FB15k-237-betae
2022-07-31 05:04:31,256 INFO     #entity: 14505
2022-07-31 05:04:31,256 INFO     #relation: 474
2022-07-31 05:04:31,256 INFO     #max steps: 150000
2022-07-31 05:04:31,256 INFO     Evaluate unoins using: DNF
2022-07-31 05:04:31,256 INFO     loading data
2022-07-31 05:05:22,795 INFO     Training info:
2022-07-31 05:05:22,799 INFO     2p: 149689
2022-07-31 05:05:25,292 INFO     Validation info:
2022-07-31 05:05:25,292 INFO     2p: 5000
2022-07-31 05:05:25,294 INFO     Test info:
2022-07-31 05:05:25,295 INFO     2p: 5000
2022-07-31 05:05:29,751 INFO     Model Parameter Configuration:
2022-07-31 05:05:29,751 INFO     Parameter embeddings.0.weight: torch.Size([14505, 2000]), require_grad = True
2022-07-31 05:05:29,751 INFO     Parameter embeddings.1.weight: torch.Size([474, 2000]), require_grad = True
2022-07-31 05:05:29,751 INFO     Parameter score_transform_fun.weight: torch.Size([2, 2000]), require_grad = True
2022-07-31 05:05:29,752 INFO     Parameter score_transform_fun.bias: torch.Size([2]), require_grad = True
2022-07-31 05:05:29,752 INFO     Parameter score_transform_ent_fun.weight: torch.Size([2, 2000]), require_grad = True
2022-07-31 05:05:29,752 INFO     Parameter score_transform_ent_fun.bias: torch.Size([2]), require_grad = True
2022-07-31 05:05:29,752 INFO     Parameter Number: 29966004
2022-07-31 05:05:29,790 INFO     Loading checkpoint models/fb15k-237-betae...
2022-07-31 05:05:30,373 INFO     tasks = 2p
2022-07-31 05:05:30,373 INFO     init_step = 99999
2022-07-31 05:05:30,373 INFO     Start Training...
2022-07-31 05:05:30,373 INFO     learning_rate = 0
2022-07-31 05:05:30,373 INFO     batch_size = 1024
2022-07-31 05:05:30,373 INFO     hidden_dim = 1000
2022-07-31 05:05:30,374 INFO     gamma = 12.000000
/home/shinaliu/miniconda3/envs/kg/lib/python3.8/site-packages/torch/autograd/__init__.py:173: UserWarning: Error detected in LogSoftmaxBackward0. Traceback of forward call that caused the error:
  File "main.py", line 632, in <module>
    main(parse_args())
  File "main.py", line 537, in main
    log = KGReasoning.train_step(model, optimizer, train_path_iterator, args, step)
  File "/home/shinaliu/KG/models/kg-reasoning-alpha/models.py", line 712, in train_step
    loss_query_type = ce_loss(all_logit, positive_sample)
  File "/home/shinaliu/miniconda3/envs/kg/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/shinaliu/miniconda3/envs/kg/lib/python3.8/site-packages/torch/nn/modules/loss.py", line 1163, in forward
    return F.cross_entropy(input, target, weight=self.weight,
  File "/home/shinaliu/miniconda3/envs/kg/lib/python3.8/site-packages/torch/nn/functional.py", line 2996, in cross_entropy
    return torch._C._nn.cross_entropy_loss(input, target, weight, _Reduction.get_enum(reduction), ignore_index, label_smoothing)
 (Triggered internally at  ../torch/csrc/autograd/python_anomaly_mode.cpp:104.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
posix.uname_result(sysname='Linux', nodename='sweetums-609-6.local', release='3.10.0-1160.49.1.el7.x86_64', version='#1 SMP Tue Nov 30 15:51:32 UTC 2021', machine='x86_64')
Traceback (most recent call last):
  File "main.py", line 632, in <module>
    main(parse_args())
  File "main.py", line 537, in main
    log = KGReasoning.train_step(model, optimizer, train_path_iterator, args, step)
  File "/home/shinaliu/KG/models/kg-reasoning-alpha/models.py", line 931, in train_step
    loss.backward()
  File "/home/shinaliu/miniconda3/envs/kg/lib/python3.8/site-packages/torch/_tensor.py", line 363, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/shinaliu/miniconda3/envs/kg/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: Function 'LogSoftmaxBackward0' returned nan values in its 0th output.
